{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U tensorboardX\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# Put your HF Token here\n",
    "os.environ['HF_TOKEN']= os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2-2b-it\"\n",
    "dataset_name = os.getenv('DATASET')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "\n",
    "def preprocess(sample):\n",
    "      messages = sample[\"messages\"]\n",
    "      first_message = messages[0]\n",
    "\n",
    "      # Instead of adding a system message, we merge the content into the first user message\n",
    "      if first_message[\"role\"] == \"system\":\n",
    "          system_message_content = first_message[\"content\"]\n",
    "          # Merge system content with the first user message\n",
    "          messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\" + messages[1][\"content\"]\n",
    "          # Remove the system message from the conversation\n",
    "          messages.pop(0)\n",
    "\n",
    "      return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.rename_column(\"conversations\", \"messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 3570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\", 'role': 'system'}, {'content': 'Hi, can you tell me the current stock price of Apple?', 'role': 'human'}, {'content': \"<think>Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\\n</tool_call>\", 'role': 'model'}, {'content': \"<tool_response>\\n{'stock_price': '$150.75'}\\n</tool_response>\", 'role': 'tool'}, {'content': 'The current stock price of Apple is $150.75.', 'role': 'model'}, {'content': \"That's great. Can you also tell me the stock price for Microsoft?\", 'role': 'human'}, {'content': \"<think>Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\\n</tool_call>\", 'role': 'model'}, {'content': \"<tool_response>\\n{'stock_price': '$210.22'}\\n</tool_response>\", 'role': 'tool'}, {'content': 'The current stock price of Microsoft is $210.22.', 'role': 'model'}, {'content': 'Thank you for the information.', 'role': 'human'}, {'content': \"You're welcome! If you have any other questions, feel free to ask.\", 'role': 'model'}]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3213\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 357\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns=\"messages\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<bos><start_of_turn>human\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions.Here are the available tools:<tools> [{\\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'get_movie_details\\', \\'description\\': \\'Get details of a movie\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'title\\': {\\'type\\': \\'string\\', \\'description\\': \\'The title of the movie\\'}, \\'year\\': {\\'type\\': \\'integer\\', \\'description\\': \\'The year of release of the movie\\'}}, \\'required\\': [\\'title\\']}}}, {\\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'calculate_tip_split\\', \\'description\\': \\'Calculate the individual tip amount for a group\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'total_bill\\': {\\'type\\': \\'number\\', \\'description\\': \\'The total bill amount\\'}, \\'number_of_people\\': {\\'type\\': \\'integer\\', \\'description\\': \\'The number of people in the group\\'}, \\'tip_percentage\\': {\\'type\\': \\'number\\', \\'description\\': \\'The percentage of tip to be given\\'}}, \\'required\\': [\\'total_bill\\', \\'number_of_people\\', \\'tip_percentage\\']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}, \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'arguments\\', \\'name\\']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nCan you tell me about the movie \"Inception\"?<end_of_turn><eos>\\n<start_of_turn>model\\n<think>Okay, I need to figure out how to respond to the user\\'s question about the movie \"Inception.\" Let me look at the tools I have available. There\\'s a function called get_movie_details that can fetch details about a movie. The user provided the title \"Inception,\" which is a required parameter for this function. I don\\'t know the release year off the top of my head, but the function only requires the title, so I can proceed without it. I\\'ll call get_movie_details with the title \"Inception\" to get the necessary information.\\n</think><tool_call>\\n{\\'name\\': \\'get_movie_details\\', \\'arguments\\': {\\'title\\': \\'Inception\\'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{\\'title\\': \\'Inception\\', \\'year\\': 2010, \\'director\\': \\'Christopher Nolan\\', \\'genre\\': \\'Science Fiction\\', \\'rating\\': 8.8}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nThe movie \"Inception\" was released in the year 2010. It was directed by Christopher Nolan and falls under the genre of Science Fiction. It has a rating of 8.8.<end_of_turn><eos>\\n<start_of_turn>human\\nWhat about \"The Dark Knight\"?<end_of_turn><eos>\\n<start_of_turn>model\\n<think>Okay, the user just asked about \"The Dark Knight.\" I remember that earlier they asked about \"Inception,\" and I used the get_movie_details function to provide the information. Since \"The Dark Knight\" is another movie, I should use the same function. The function requires the title, which is \"The Dark Knight,\" so I don\\'t need any additional parameters. I\\'ll call get_movie_details with that title to fetch the details and respond accurately.\\n</think><tool_call>\\n{\\'name\\': \\'get_movie_details\\', \\'arguments\\': {\\'title\\': \\'The Dark Knight\\'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{\\'title\\': \\'The Dark Knight\\', \\'year\\': 2008, \\'director\\': \\'Christopher Nolan\\', \\'genre\\': \\'Action, Crime, Drama\\', \\'rating\\': 9.0}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\n\"The Dark Knight\" was released in 2008. It was also directed by Christopher Nolan and is an Action, Crime, Drama movie. It has a rating of 9.0.<end_of_turn><eos>\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details of a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}, 'year': {'type': 'integer', 'description': 'The year of release of the movie'}}, 'required': ['title']}}}, {'type': 'function', 'function': {'name': 'calculate_tip_split', 'description': 'Calculate the individual tip amount for a group', 'parameters': {'type': 'object', 'properties': {'total_bill': {'type': 'number', 'description': 'The total bill amount'}, 'number_of_people': {'type': 'integer', 'description': 'The number of people in the group'}, 'tip_percentage': {'type': 'number', 'description': 'The percentage of tip to be given'}}, 'required': ['total_bill', 'number_of_people', 'tip_percentage']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "Can you tell me about the movie \"Inception\"?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, I need to figure out how to respond to the user's question about the movie \"Inception.\" Let me look at the tools I have available. There's a function called get_movie_details that can fetch details about a movie. The user provided the title \"Inception,\" which is a required parameter for this function. I don't know the release year off the top of my head, but the function only requires the title, so I can proceed without it. I'll call get_movie_details with the title \"Inception\" to get the necessary information.\n",
      "</think><tool_call>\n",
      "{'name': 'get_movie_details', 'arguments': {'title': 'Inception'}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'genre': 'Science Fiction', 'rating': 8.8}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "The movie \"Inception\" was released in the year 2010. It was directed by Christopher Nolan and falls under the genre of Science Fiction. It has a rating of 8.8.<end_of_turn><eos>\n",
      "<start_of_turn>human\n",
      "What about \"The Dark Knight\"?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, the user just asked about \"The Dark Knight.\" I remember that earlier they asked about \"Inception,\" and I used the get_movie_details function to provide the information. Since \"The Dark Knight\" is another movie, I should use the same function. The function requires the title, which is \"The Dark Knight,\" so I don't need any additional parameters. I'll call get_movie_details with that title to fetch the details and respond accurately.\n",
      "</think><tool_call>\n",
      "{'name': 'get_movie_details', 'arguments': {'title': 'The Dark Knight'}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'title': 'The Dark Knight', 'year': 2008, 'director': 'Christopher Nolan', 'genre': 'Action, Crime, Drama', 'rating': 9.0}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "\"The Dark Knight\" was released in 2008. It was also directed by Christopher Nolan and is an Action, Crime, Drama movie. It has a rating of 9.0.<end_of_turn><eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaTokenizerFast(name_or_path='google/gemma-2-2b-it', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t5: AddedToken(\"<2mass>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t6: AddedToken(\"[@BOS@]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t7: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t8: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t9: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t10: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t11: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t12: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t13: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t14: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t15: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t16: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t17: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t18: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t19: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t20: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t21: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t22: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t23: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t24: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t25: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t26: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t27: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t28: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t29: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t30: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t31: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t33: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t34: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t35: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t36: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t37: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t38: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t39: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t40: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t41: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t42: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t43: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t44: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t45: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t46: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t47: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t48: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t49: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t50: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t51: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t52: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t53: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t54: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t55: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t56: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t57: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t58: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t59: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t60: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t61: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t62: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t63: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t64: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t65: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t66: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t67: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t68: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t69: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t70: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t71: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t72: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t73: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t74: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t75: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t76: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t77: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t78: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t79: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t80: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t81: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t82: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t83: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t84: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t85: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t86: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t87: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t88: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t89: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t90: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t91: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t92: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t93: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t94: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t95: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t96: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t97: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t98: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t99: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t100: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t101: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t102: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t103: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t104: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t105: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t106: AddedToken(\"<start_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t107: AddedToken(\"<end_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t108: AddedToken(\"\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t109: AddedToken(\"\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t110: AddedToken(\"\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t111: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t112: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t113: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t114: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t115: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t116: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t117: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t118: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t119: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t120: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t121: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t122: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t123: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t124: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t125: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t126: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t127: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t128: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t129: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t130: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t131: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t132: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t133: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t134: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t135: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t136: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t137: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t138: AddedToken(\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t139: AddedToken(\"▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t140: AddedToken(\"▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t141: AddedToken(\"▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t142: AddedToken(\"▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t143: AddedToken(\"▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t144: AddedToken(\"▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t145: AddedToken(\"▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t146: AddedToken(\"▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t147: AddedToken(\"▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t148: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t149: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t150: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t152: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t153: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t154: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t155: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t156: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t157: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t158: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t159: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t160: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t161: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t162: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t163: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t164: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t165: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t166: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t167: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t168: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t169: AddedToken(\"<table>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t170: AddedToken(\"<caption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t171: AddedToken(\"<thead>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t172: AddedToken(\"<tbody>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t173: AddedToken(\"<tfoot>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t174: AddedToken(\"<tr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t175: AddedToken(\"<th>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t176: AddedToken(\"<td>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t177: AddedToken(\"</table>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t178: AddedToken(\"</caption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t179: AddedToken(\"</thead>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t180: AddedToken(\"</tbody>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t181: AddedToken(\"</tfoot>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t182: AddedToken(\"</tr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t183: AddedToken(\"</th>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t184: AddedToken(\"</td>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t185: AddedToken(\"<h1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t186: AddedToken(\"<h2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t187: AddedToken(\"<h3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t188: AddedToken(\"<h4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t189: AddedToken(\"<h5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t190: AddedToken(\"<h6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t191: AddedToken(\"<blockquote>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t192: AddedToken(\"</h1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t193: AddedToken(\"</h2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t194: AddedToken(\"</h3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t195: AddedToken(\"</h4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t196: AddedToken(\"</h5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t197: AddedToken(\"</h6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t198: AddedToken(\"</blockquote>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t199: AddedToken(\"<strong>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t200: AddedToken(\"<em>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t201: AddedToken(\"<b>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t202: AddedToken(\"<i>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t203: AddedToken(\"<u>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t204: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t205: AddedToken(\"<sub>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t206: AddedToken(\"<sup>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t207: AddedToken(\"<code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t208: AddedToken(\"</strong>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t209: AddedToken(\"</em>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t210: AddedToken(\"</b>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t211: AddedToken(\"</i>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t212: AddedToken(\"</u>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t213: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t214: AddedToken(\"</sub>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t215: AddedToken(\"</sup>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t216: AddedToken(\"</code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255968: AddedToken(\"[toxicity=0]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255969: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255970: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255971: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255972: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255973: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255974: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255975: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255976: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255977: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255978: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255979: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255980: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255981: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255982: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255983: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255984: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255985: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255986: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255987: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255988: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255989: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255990: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255991: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255992: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255993: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255994: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255995: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255996: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255997: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255998: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t255999: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8f4f88a74b47bd9fa8a1e28abcc64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(256008, 2304, padding_idx=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call=\"<tool_call>\"\n",
    "    eotool_call=\"</tool_call>\"\n",
    "    tool_response=\"<tool_reponse>\"\n",
    "    eotool_response=\"</tool_reponse>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "        additional_special_tokens=ChatmlSpecialTokens.list()\n",
    "    )\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             attn_implementation='eager',\n",
    "                                             device_map=\"cuda:0\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model.to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256008, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256008, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 16\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 64\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(r=rank_dimension,\n",
    "                         lora_alpha=lora_alpha,\n",
    "                         lora_dropout=lora_dropout,\n",
    "                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"], # wich layer in the transformers do we target ?\n",
    "                         task_type=TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "username=\"langdai\"\n",
    "output_dir = \"gemma-2-2B-it_think_funcion_call\"\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "logging_steps = 5\n",
    "learning_rate = 1e-4 # The initial learning rate for the optimizer.\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs=1\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 1500\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    bf16=False,\n",
    "    hub_private_repo=False,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    packing=True,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='533' max='533' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [533/533 2:21:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.294032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f597cf2a19a446186e6d16a40b36488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453e5f5d3c594319885ab5d4c914a3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1743239713.ip-10-192-11-217.1523.0:   0%|          | 0.00/41.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e0321873414dbe8cfdb192e018d82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf25ed058be4318ab71fc8d85cf949f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838d2c1f696a447bb2eea2fcc4f4cd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4d0f46b36a4219845e8dfeb261c246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/langdai/gemma-2-2B-it_think_funcion_call/commit/f307db9893560e251fb35b126e551f8adad7278c', commit_message='Upload tokenizer', commit_description='', oid='f307db9893560e251fb35b126e551f8adad7278c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/langdai/gemma-2-2B-it_think_funcion_call', endpoint='https://huggingface.co', repo_type='model', repo_id='langdai/gemma-2-2B-it_think_funcion_call'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Push to huggingface\n",
    "\n",
    "# trainer.push_to_hub(f\"{username}/{output_dir}\")\n",
    "\n",
    "\n",
    "# tokenizer.eos_token = \"<eos>\"\n",
    "# tokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d73e7d5092b4c2aaaa9facdaf1fb432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): lora.Embedding(\n",
       "          (base_layer): Embedding(256008, 2304, padding_idx=0)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 16x256008 (cuda:0)])\n",
       "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 2304x16 (cuda:0)])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9216, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): lora.Linear(\n",
       "        (base_layer): Linear(in_features=2304, out_features=256008, bias=False)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=16, out_features=256008, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "username=\"langdai\"\n",
    "output_dir = \"gemma-2-2B-it_think_funcion_call\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "peft_model_id = f\"{username}/{output_dir}\"\n",
    "# device = \"auto\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                             device_map=\"cuda:0\",\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.to(torch.bfloat16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Okay, so the user is asking to convert 500 USD to Euros. I need to figure out how to respond using the available functions. Let me look at the tools provided. There's a function called 'convert_currency' which seems perfect for this task. It requires an amount, the source currency, and the target currency. The user provided all these details: 500 USD to EUR. \n",
      "\n",
      "I should structure the function call with the correct parameters. The amount is 500, from_currency is USD, and to_currency is EUR. I need to make sure the JSON format is correct, including the function name and the arguments object. \n",
      "\n",
      "I think that's all I need to do. The function should handle the conversion accurately.\n",
      "</think><tool_call>\n",
      "{'name': 'convert_currency', 'arguments': {'amount': 500, 'from_currency': 'USD', 'to_currency': 'EUR'}}\n",
      "</tool_call><end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "#this prompt is a sub-sample of one of the test set examples. In this example we start the generation after the model generation starts.\n",
    "prompt=\"\"\"<bos><start_of_turn>human\n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert from one currency to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The currency to convert from'}, 'to_currency': {'type': 'string', 'description': 'The currency to convert to'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}, {'type': 'function', 'function': {'name': 'calculate_distance', 'description': 'Calculate the distance between two locations', 'parameters': {'type': 'object', 'properties': {'start_location': {'type': 'string', 'description': 'The starting location'}, 'end_location': {'type': 'string', 'description': 'The ending location'}}, 'required': ['start_location', 'end_location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{tool_call}\n",
    "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
    "\n",
    "Hi, I need to convert 500 USD to Euros. Can you help me with that?<end_of_turn><eos>\n",
    "<start_of_turn>model\n",
    "<think>\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=300,# Adapt as necessary\n",
    "                         do_sample=True,\n",
    "                         top_p=0.95,\n",
    "                         temperature=0.01,\n",
    "                         repetition_penalty=1.0,\n",
    "                         eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1d9146418c4097b5059d376aa90911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "peft_model_id = \"langdai/gemma-2-2B-it_think_funcion_call\"\n",
    "# device = \"auto\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                             device_map=\"cuda:0\",\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.to(torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "generator = pipeline(\"text-generation\", model= model, tokenizer= tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\n",
    "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a fascinating thought experiment! If I had a time machine that could only travel to the past or the future once, I would choose to go to the future. Here's why:\n",
      "\n",
      "1. **The Unknown:** The future is a realm of possibilities. It's filled with potential advancements, discoveries, and events that we can't even imagine yet. By going to the future, I could witness these advancements firsthand and gain a deeper understanding of the world.\n",
      "\n",
      "2. **The Impact:** The past is often romanticized, but it's also a time of great loss and hardship. By going to the\n"
     ]
    }
   ],
   "source": [
    "print(output[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
